# ğŸ—£ï¸ Mini Voice Assistant

A simple voice assistant that:
- Listens to your speech via microphone ğŸ™ï¸
- Transcribes it using OpenAI Whisper ğŸ§ 
- Sends the text to a language model ğŸ¤–
- Speaks the response out loud ğŸ”Š

---

## ğŸ”§ Features

- ğŸ›ï¸ Input device selection
- ğŸ™ï¸ Audio recording with `sounddevice`
- ğŸ§  Transcription using Whisper
- ğŸ’¬ Text-to-text response from LLM (e.g., OpenAI GPT-3.5)
- ğŸ”Š Speech playback via `pyttsx3`

---

## ğŸ“¦ Requirements

Install all dependencies using:

```bash
pip install -r requirements.txt
```

## FFmpeg (Required for Whisper)
- Whisper requires FFmpeg to process audio.

### ğŸ‘‰ Install FFmpeg:
- Download from: https://ffmpeg.org/download.html

- Extract the ZIP archive

- Add the bin/ folder path (e.g., C:\ffmpeg\bin) to your System PATH


## LLM API Key
- This project is compatible with any LLM provider that supports chat-style completions.

- You must provide your own API key in llm.py.

### Example using OpenAI:

```bash
from openai import OpenAI
client = OpenAI(api_key="sk-...")  # Replace with your key
```

## How to Use
1. Clone this repo
```bash
git clone https://github.com/YOUR_USERNAME/mini_voice_assistant.git
cd mini_voice_assistant
```
2. Set up virtual environment
```bash
python -m venv venv
venv\Scripts\activate  # (Windows)
```
3. Install dependencies
```bash
pip install -r requirements.txt
```
4. Run the app
```bash
python main.py
```
### The assistant will:

1. List available input devices

2. Let you select your microphone

3. Record your voice

4. Transcribe it using Whisper

5. Get a response from the LLM

6. Speak it aloud using TTS

## Project Structure

mini_voice_assistant\
â”œâ”€â”€ main.py        # Entry point\
â”œâ”€â”€ stt.py         # Speech-to-text logic\
â”œâ”€â”€ llm.py         # Language model communication\
â”œâ”€â”€ tts.py         # Text-to-speech\
â”œâ”€â”€ requirements.txt\
â”œâ”€â”€ README.md\
â””â”€â”€ .gitignore
